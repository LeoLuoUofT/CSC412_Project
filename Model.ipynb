{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HG5MA9DB1vD6"
      },
      "source": [
        "# Gaussian Mixture VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_NvEAosY_Wg"
      },
      "source": [
        "Many utilities borrowed from https://github.com/jariasf/GMVAE/blob/master/pytorch/networks/Layers.py. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJlMOIXC135X"
      },
      "source": [
        "## Neural Network Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMCUUPqdF9N4"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYIrwlLKULu8"
      },
      "source": [
        "def get_activation_function(id):\n",
        "  if id == 'relu':\n",
        "    return nn.ReLU()\n",
        "  \n",
        "  if id == 'sigmoid':\n",
        "    return nn.Sigmoid()\n",
        "\n",
        "  if id == 'tanh':\n",
        "    return nn.Tanh()\n",
        "\n",
        "  if id == 'none':\n",
        "    return nn.Identity()\n",
        "\n",
        "  else:\n",
        "    raise ValueError\n",
        "\n",
        "def get_batch_norm(bool, size):\n",
        "  if bool:\n",
        "    return nn.BatchNorm1d(size)\n",
        "\n",
        "  else:\n",
        "    return nn.Identity()\n",
        "\n",
        "def get_rnn_cell(id):\n",
        "  # returns constructor\n",
        "  if id == 'gru':\n",
        "    return nn.GRU\n",
        "\n",
        "  if id == 'lstm':\n",
        "    return nn.LSTM  \n",
        "\n",
        "  if id == 'basic':\n",
        "    return nn.RNN # why use this\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DISAE6n11tZa"
      },
      "source": [
        "class ff(nn.Module):\n",
        "    def __init__(self, argdict):\n",
        "      \"\"\"\n",
        "      argdict: contains all arguments \n",
        "      \"\"\"\n",
        "      super(ff, self).__init__()\n",
        "\n",
        "      self.argdict = argdict\n",
        "\n",
        "      input_dim = self.argdict[\"input_dim\"]\n",
        "      output_dim = self.argdict[\"output_dim\"]\n",
        "      layer_params = self.argdict[\"layer_params\"]\n",
        "      n_layers = len(layer_params)\n",
        "\n",
        "      \n",
        "      self.layers = []\n",
        "      \n",
        "      if n_layers != 0:\n",
        "        self.layers.append(\n",
        "          nn.Sequential(\n",
        "            nn.Linear(input_dim, layer_params[0][\"size\"]),\n",
        "            get_batch_norm(self.argdict[0][\"batch_norm\"], layer_params[0][\"size\"]),\n",
        "            get_activation_function(layer_params[0][\"activation_fn\"])\n",
        "          )\n",
        "        )\n",
        "      \n",
        "        for i in range(n_layers-1):\n",
        "          self.layers.append(\n",
        "            nn.Sequential(\n",
        "              nn.Linear(layer_params[i][\"size\"], layer_params[i+1][\"size\"]),\n",
        "              get_batch_norm(self.argdict[i+1][\"batch_norm\"], layer_params[i+1][\"size\"]),\n",
        "              get_activation_function(layer_params[i+1][\"activation_fn\"])\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        self.layers.append(\n",
        "            nn.Sequential(\n",
        "              nn.Linear(layer_params[-1][\"size\"], output_dim),\n",
        "              get_activation_function(self.argdict[\"output_activation_fn\"])\n",
        "            )\n",
        "        )\n",
        "\n",
        "      # 0 layer case, just pipe to output\n",
        "      else:\n",
        "        self.layers.append(\n",
        "            nn.Sequential(\n",
        "              nn.Linear(input_dim, output_dim),\n",
        "              get_activation_function(self.argdict[\"output_activation_fn\"])\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "      for layer in self.layers:\n",
        "        x = layer(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "class rnn(nn.Module):\n",
        "    def __init__(self, argdict):\n",
        "      \"\"\"\n",
        "      \"\"\"\n",
        "      super(rnn, self).__init__()\n",
        "\n",
        "      self.argdict = argdict\n",
        "\n",
        "      # no embedding? assume already embedded\n",
        "\n",
        "      input_dim = self.argdict[\"input_dim\"]\n",
        "      output_dim = self.argdict[\"output_dim\"]\n",
        "\n",
        "      rnn_cell_constructor = get_rnn_cell(self.argdict[\"rnn_cell\"])\n",
        "\n",
        "      self.rnn_layer = rnn_cell_constructor(input_dim, output_dim, batch_first = True)\n",
        "\n",
        "    def forward(self, x):\n",
        "      return self.rnn_layer(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbCN9IjPChPA"
      },
      "source": [
        "# sampling\n",
        "def gumbel_sampler(x, temperature)\n",
        "    # softmax but with noise\n",
        "    sampled = torch.rand(x.size())\n",
        "    eps = 1e-10 # stability\n",
        "    if x.is_cuda:\n",
        "      sampled = sampled.cuda()\n",
        "    noise = torch.log(-torch.log(sampled + eps) + eps) # loglog\n",
        "    return F.softmax((x - noise) / temperature, dim=-1)\n",
        "\n",
        "def gaussian_sampler(m, v):\n",
        "    std = torch.sqrt(v + 1e-10)\n",
        "    eps = torch.randn_like(std)\n",
        "    z = m + eps * std\n",
        "    return z\n",
        "\n",
        "# losses\n",
        "def cross_entropy(logits, labels):\n",
        "    return F.cross_entropy(logits, labels)\n",
        "\n",
        "def mse(pred, labels):\n",
        "    loss = (pred - labels).pow(2)\n",
        "    return loss.sum(-1).mean()\n",
        "\n",
        "def entropy(logits, labels):\n",
        "    # wrt logits\n",
        "    log_q = F.log_softmax(logits, dim=-1)\n",
        "    return -torch.mean(torch.sum(labels * log_q, dim=-1))\n",
        "\n",
        "def log_normal(z, m, v):\n",
        "    v_stable = v + 1e-10\n",
        "    return -0.5 * torch.sum(torch.pow(z - m, 2)/v + torch.log(var), dim=-1) # ignore constant \n",
        "\n",
        "def gaussian_kl(sample, mu, var, mu_prior, var_prior):\n",
        "    loss = log_normal(sample, mu, var) - log_normal(sample, mu_prior, var_prior)\n",
        "    return loss.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlP_umuVKiEO"
      },
      "source": [
        "class softmax_with_gumbel(nn.Module):\n",
        "\n",
        "  def __init__(self, argdict):\n",
        "    super(softmax_with_gumbel, self).__init__()\n",
        "    \n",
        "    self.argdict = argdict\n",
        "\n",
        "    self.input_dim = argdict[\"input_dim\"]\n",
        "    self.output_dim = argdict[\"output_dim\"]\n",
        "    self.layer = nn.Linear(input_dim, output_dim)\n",
        "    self.activation = nn.Softmax(dim = -1)\n",
        "  \n",
        "  def forward(self, x, temperature = 1.0):\n",
        "    x = self.layer(x)\n",
        "    y = gumbel_sampler(x, temperature)\n",
        "    return self.activation(x), y # logits, y\n",
        "\n",
        "class gaussian(nn.Module):\n",
        "  def __init__(self, argdict):\n",
        "    super(gaussian, self).__init__()\n",
        "\n",
        "    self.argdict = argdict\n",
        "    self.input_dim = argdict[\"input_dim\"]\n",
        "    self.output_dim = argdict[\"output_dim\"]\n",
        "\n",
        "    self.mu_layer = nn.Linear(self.input_dim, self.output_dim)\n",
        "    self.var_layer = nn.Sequential(\n",
        "        nn.Linear(self.input_dim, self.output_dim),\n",
        "        nn.Softplus() # need softplus\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    mu = self.mu_layer(x)\n",
        "    var = self.var(x)\n",
        "    z = gaussian_sampler(mu, var)\n",
        "    return mu, var, z    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLuLzcyBGOnn"
      },
      "source": [
        "class encoder(nn.Module):\n",
        "  def __init__(self, argdict):\n",
        "    super(encoder, self).__init__()\n",
        "\n",
        "    self.argdict = argdict\n",
        "    \n",
        "    # q(y|x)\n",
        "    self.q_y_network = torch.nn.Sequential(\n",
        "      rnn(argdict[\"q_y_rnn\"]), # rnn component\n",
        "      ff(argdict[\"q_y_linear\"])                                    \n",
        "    ) # make sure to constrain arguments\n",
        "\n",
        "    self.q_y = softmax_with_gumbel(argdict[\"q_y_gumbel\"]) # separate for temperature parameter\n",
        "    \n",
        "    # q(z|y,x)\n",
        "    self.q_z = torch.nn.Sequential(\n",
        "      rnn(argdict[\"q_z_rnn\"]), # rnn?? can remove if buggy\n",
        "      ff(argdict[\"q_z_linear\"]),\n",
        "      gaussian(argdict[\"q_z_gaussian\"])                                          \n",
        "    ) # make sure to constrain arguments\n",
        "  \n",
        "  def forward_fixed_y(self, x, y_fixed):\n",
        "    # for style transfer\n",
        "    \n",
        "    mu, var, z = self.q_z(torch.cat((x, y_fixed), dim=1))\n",
        "    return_dict = {'mu': mu, 'var': var, 'z': z}\n",
        "    return return_dict\n",
        "\n",
        "  def forward(self, x, temperature = 1.0):\n",
        "    pi, y = self.q_y(self.q_y_network(x), temperature = temperature)\n",
        "    \n",
        "    mu, var, z = self.q_z(torch.cat((x, y), dim=-1))\n",
        "    \n",
        "    return_dict = {'pi': pi, 'y': y, 'mu': mu, 'var': var, 'z': z}\n",
        "    return return_dict\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, argdict):\n",
        "    super(decoder, self).__init__()\n",
        "\n",
        "    self.argdict = argdict\n",
        "    input_dim = self.argdict[\"input_dim\"] # y_dim\n",
        "    output_dim = self.argdict[\"output_dim\"] # z_dim\n",
        "    # make sure dims match when constructing args\n",
        "\n",
        "    self.p_z_mu_nn = nn.Linear(input_dim, output_dim)\n",
        "    self.p_z_var_nn = nn.Sequential(\n",
        "      nn.Linear(input_dim, output_dim),\n",
        "      nn.Softplus()\n",
        "    )\n",
        "\n",
        "    self.p_x = torch.nn.Sequential(\n",
        "      ff(argdict[\"p_x_linear\"]),\n",
        "      rnn(argdict[\"p_x_rnn\"]),\n",
        "    ) # apply activations on output\n",
        "\n",
        "\n",
        "  def forward(self, z, y):\n",
        "    z_mu = self.p_z_mu_nn(y)\n",
        "    z_var = self.p_z_var_nn(y)\n",
        "    \n",
        "    x = self.p_x(z)\n",
        "\n",
        "    return_dict = {'mu': y_mu, 'var': y_var, 'x': x}\n",
        "    return return_dict\n",
        "\n",
        "class GMVAE(nn.Module):\n",
        "  def __init__(self, argdict):\n",
        "    super(GMVAE, self).__init__()\n",
        "\n",
        "    self.encoder = encoder(argdict[\"encoder\"])\n",
        "    self.decoder = decoder(argdict[\"decoder\"])\n",
        "\n",
        "    # weight initialization\n",
        "    for m in self.modules():\n",
        "      if type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_normal_(m.weight)\n",
        "      elif type(m) == nn.RNN or type(m) == nn.GRU or type(m) == nn.LSTM:\n",
        "        torch.nn.init.he_normal(m.weight) # RNN weighting\n",
        "\n",
        "  def style_transfer(self, x, y_fixed):\n",
        "    # y_fixed should be binary vector with the target style\n",
        "\n",
        "    encoder_returns = self.encoder.forward_fixed_y(x, y_fixed)\n",
        "    z = encoder_returns['z']\n",
        "    decoder_returns = self.decoder(z, y_fixed)\n",
        "    \n",
        "    return_dict = {\"encoder\": encoder_returns, \"decoder\": decoder_returns}\n",
        "    return return_dict\n",
        "\n",
        "  def forward(self, x, temperature=1.0):\n",
        "    # standard\n",
        "\n",
        "    encoder_returns = self.encoder(x, temperature = temperature)\n",
        "    z, y = encoder_returns['z'], encoder_returns['y']\n",
        "    decoder_returns = self.decoder(z, y)\n",
        "    \n",
        "    return_dict = {\"encoder\": encoder_returns, \"decoder\": decoder_returns}\n",
        "    return return_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWQ0AndtiVeY"
      },
      "source": [
        "class Model():\n",
        "  def __init__(self, argdict):\n",
        "    # unpacking 50000000 args\n",
        "    # lr control\n",
        "\n",
        "    # lr variable\n",
        "    self.learning_rate = argdict[\"learning_rate\"]\n",
        "    \n",
        "    # decay parameters\n",
        "    self.decay_epoch = argdict[\"decay_epoch\"]\n",
        "    self.lr_decay = argdict[\"lr_decay\"]\n",
        "\n",
        "    # weighting for loss\n",
        "    self.weight_style = argdict[\"weight_style\"]\n",
        "    self.weight_entropy = argdict[\"weight_entropy\"]\n",
        "    self.weight_sampling = argdict[\"weight_sampling\"]\n",
        "\n",
        "    # mix different audio??   \n",
        "    self.weight_pitch = argdict[\"weight_pitch\"]\n",
        "    self.weight_instrument = argdict[\"weight_instrument\"]\n",
        "    self.weight_velocity = argdict[\"weight_velocity\"]\n",
        "\n",
        "    # sizes, make sure it matches\n",
        "    self.pitch_size = argdict[\"pitch_size\"]\n",
        "    self.instrument_size = argdict[\"instrument_size\"]\n",
        "    self.velocity_size = argdict[\"velocity_size\"]\n",
        "\n",
        "    # temperature for sampling for GMM, very annoying\n",
        "    self.init_temp = argdict[\"init_temp\"]\n",
        "    self.decay_temp = argdict[\"decay_temp\"]\n",
        "    self.min_temp = argdict[\"min_temp\"]\n",
        "    self.decay_temp_rate = argdict[\"decay_temp_rate\"]\n",
        "\n",
        "    # temperature variable\n",
        "    self.gumbel_temp = self.init_temp\n",
        "\n",
        "    self.model = GMVAE(argdict)\n",
        "    if argdict[\"cuda\"]:\n",
        "      self.model = self.model.cuda()\n",
        "\n",
        "  def _elbo(self, pitch, instrument, velocity, style_label):\n",
        "    x = torch.cat(pitch, instrument, velocity, dim=-1) # should be dim = 3\n",
        "    return_dict = self.model(x, temperature = self.gumbel_temp)\n",
        "\n",
        "    x_pred = return_dict[\"decoder\"][\"x\"]\n",
        "    pitch_pred, instrument_pred, velocity_pred = \\\n",
        "      torch.split(x_pred, [self.pitch_size, self.instrument_size, self.velocity_size], dim=-1)\n",
        "\n",
        "    # renormalizing?\n",
        "    pitch_pred = (pitch_pred + 1)/2\n",
        "    instrument_pred = (instrument_pred + 1)/2\n",
        "    velocity_pred = (velocity_pred + 1)/2 + 0.5 # is this correct?\n",
        "\n",
        "    loss_pitch = cross_entropy(pitch_pred, torch.argmax(pitch, dim = -1))\n",
        "    loss_instrument = cross_entropy(instrument_pred, torch.argmax(instrument, dim = -1))\n",
        "    loss_velocity = mse(velocity, velocity_pred)\n",
        "\n",
        "    style_logits = return_dict[\"encoder\"][\"pi\"]\n",
        "    y_pred = return_dict[\"encoder\"][\"y\"]\n",
        "\n",
        "    loss_style = cross_entropy(style_logits, torch.argmax(style_label, dim=-1)) # style label is 1-hot?\n",
        "    loss_entropy = entropy(style_logits, y_pred)\n",
        "\n",
        "    z_pred = return_dict[\"encoder\"][\"z\"]\n",
        "    new_mu, new_var = return_dict[\"encoder\"][\"mu\"], return_dict[\"encoder\"][\"var\"]\n",
        "    old_mu, old_var = return_dict[\"decoder\"][\"mu\"], return_dict[\"decoder\"][\"var\"]\n",
        "\n",
        "    loss_kl = gaussian_kl(z_pred, new_mu, new_var, old_mu, old_var)\n",
        "\n",
        "    loss_total = loss_pitch * self.weight_pitch + \\\n",
        "      loss_instrument * self.weight_instrument + loss_velocity * self.weight_velocity + \\\n",
        "      loss_style * self.weight_style + loss_entropy * self.weight_entropy + \\\n",
        "      loss_kl * self.weight_sampling\n",
        "\n",
        "    stats_dict = {'kl': loss_kl, 'entropy': loss_entropy, 'ce_style': loss_style,\n",
        "                  'ce_pitch': loss_pitch, 'ce_instrument': loss_instrument,\n",
        "                  'mse_velocity': loss_velocity, 'total': loss_total}\n",
        "\n",
        "  def _step(self):\n",
        "    # get _elbo, optimize\n",
        "\n",
        "  def train(self, data_loader):\n",
        "    # iterate on data loader using step\n",
        "\n",
        "    # decay lr and temp appropriately\n",
        "\n",
        "  def test(self, data_loader):\n",
        "    # iterate\n",
        "\n",
        "  def run(self, train_loader, test_loader):\n",
        "    # train, test then plot outputs?? save model somehow\n",
        "\n",
        "  def transfer(self, data_loader):\n",
        "    # use the style transfer function in GMVAE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTPpdI1ueHtR"
      },
      "source": [
        "def load_data():\n",
        "  # returns data\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-mInos7mMhj"
      },
      "source": [
        "# shared constants\n",
        "\n",
        "PITCH_DIM = ???\n",
        "INSTRUMENT_DIM = ???\n",
        "VELOCITY_DIM = ???\n",
        "\n",
        "NUM_STYLES = ???\n",
        "LATENT_DIM = 128 # ??\n",
        "\n",
        "RNN_CELL = \"gru\"\n",
        "RNN_CELL_NUMBER = LATENT_DIM # this is a lot\n",
        "\n",
        "# all hyperparameters in here lol\n",
        "# sorry, hope this is easy to read\n",
        "encoder_dict = {\n",
        "    \"q_y_rnn\": {\"input_dim\": PITCH_DIM + INSTRUMENT_DIM + VELOCITY_DIM,\n",
        "                \"output_dim\": 3 * LATENT_DIM,\n",
        "                \"rnn_cell\": RNN_CELL},\n",
        "    \"q_y_linear\": {\"input_dim\": 3 * LATENT_DIM,\n",
        "                   \"output_dim\": LATENT_DIM,\n",
        "                   \"output_activation_function\": \"relu\"},\n",
        "    \"q_y_gumbel\": {\"input_dim\": LATENT_DIM,\n",
        "                   \"output_dim\": NUM_STYLES},\n",
        "\n",
        "    \"q_z_rnn\": {\"input_dim\": PITCH_DIM + INSTRUMENT_DIM + VELOCITY_DIM + NUM_STYLES,\n",
        "                \"output_dim\": 3 * LATENT_DIM,\n",
        "                \"rnn_cell\": RNN_CELL},\n",
        "    \"q_z_linear\": {\"input_dim\": 3 * LATENT_DIM,\n",
        "                   \"output_dim\": LATENT_DIM,\n",
        "                   \"output_activation_function\": \"relu\"},\n",
        "    \"q_z_gaussian\": {\"input_dim\": LATENT_DIM,\n",
        "                     \"output_dim\": LATENT_DIM},\n",
        "}\n",
        "\n",
        "decoder_dict = {\n",
        "    \"input_dim\": NUM_STYLES,\n",
        "    \"output_dim\": LATENT_DIM,\n",
        "\n",
        "    \"p_x_linear\": {\"input_dim\": LATENT_DIM,\n",
        "                   \"output_dim\": 3 * LATENT_DIM,\n",
        "                   \"output_activation_function\": \"tanh\"}, # tanh better for rnn?\n",
        "    \"p_x_rnn\": {\"input_dim\": 3 * LATENT_DIM,\n",
        "                \"output_dim\": PITCH_DIM + INSTRUMENT_DIM + VELOCITY_DIM,\n",
        "                \"rnn_cell\": RNN_CELL},\n",
        "}\n",
        "\n",
        "argdict = {\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"decay_epoch\": 100,\n",
        "    \"lr_decay\": 1e-1,\n",
        "\n",
        "    # tune these\n",
        "    \"weight_style\": 1,\n",
        "    \"weight_entropy\": 0.5,\n",
        "    \"weight_sampling\": 1,\n",
        "\n",
        "    \"weight_pitch\": 1,\n",
        "    \"weight_velocity\": 1,\n",
        "    \"weight_instrument\": 1,\n",
        "\n",
        "    \"pitch_size\": PITCH_DIM, #idk,\n",
        "    \"instrument_size\": INSTRUMENT_DIM, # idk\n",
        "    \"velocity_size\": VELOCITY_DIM,\n",
        "\n",
        "    \"init_temp\": 1e-1,\n",
        "    \"decay_temp\": 1e-1,\n",
        "    \"min_temp\": 1e-5,\n",
        "    \"decay_temp_rate\": 5, # every N epochs\n",
        "\n",
        "    \"cuda\": True, # use GPU\n",
        "\n",
        "    \"encoder\": encoder_dict,\n",
        "    \"decoder\": decoder_dict,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8iHkNpV1LFv"
      },
      "source": [
        "# Algorithm pipeline\n",
        "\n",
        "data_loader = load_data() # x and y's\n",
        "\n",
        "model = Model(argdict)\n",
        "model.run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}